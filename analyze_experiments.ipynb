{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### comparing whole result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Dataset Experiment  Train Balanced Accuracy  \\\n",
      "0        cll        dnn                 0.908107   \n",
      "1        cll        nmf                 0.889562   \n",
      "2        cll        svd                 0.464659   \n",
      "3       lung        dnn                 0.702771   \n",
      "4       lung        nmf                 0.901653   \n",
      "5       lung        svd                 0.475263   \n",
      "6   prostate        dnn                 0.878824   \n",
      "7   prostate        nmf                 0.938039   \n",
      "8   prostate        svd                 0.685882   \n",
      "9        smk        dnn                 0.631334   \n",
      "10       smk        nmf                 0.722183   \n",
      "11       smk        svd                 0.505000   \n",
      "12  toxicity        dnn                 0.611777   \n",
      "13  toxicity        nmf                 0.773102   \n",
      "14  toxicity        svd                 0.417312   \n",
      "\n",
      "    Validation Balanced Accuracy  Test Balanced Accuracy  \n",
      "0                       0.833333                0.722896  \n",
      "1                       0.733333                0.708687  \n",
      "2                       0.466667                0.453333  \n",
      "3                       0.668182                0.716071  \n",
      "4                       0.868182                0.872421  \n",
      "5                       0.434091                0.451786  \n",
      "6                       0.865000                0.796364  \n",
      "7                       0.830000                0.822727  \n",
      "8                       0.630000                0.600000  \n",
      "9                       0.542857                0.562865  \n",
      "10                      0.587500                0.656637  \n",
      "11                      0.500000                0.494737  \n",
      "12                      0.575000                0.520139  \n",
      "13                      0.700000                0.638194  \n",
      "14                      0.320833                0.392361  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "datasets = ('lung', 'prostate', 'toxicity', 'cll', 'smk')\n",
    "experiment_types = ('svd', 'nmf', 'dnn')\n",
    "\n",
    "experiment_results = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    for experiment_type in experiment_types:\n",
    "        experiment_name = f\"{experiment_type}_{dataset}\"\n",
    "        folder_path = os.path.join('logs', experiment_name)\n",
    "\n",
    "        for folder in os.listdir(folder_path):\n",
    "            metrics_path = os.path.join(folder_path, folder, 'metrics.csv')\n",
    "            if os.path.exists(metrics_path):\n",
    "                metrics = pd.read_csv(metrics_path)\n",
    "\n",
    "                train_balanced_acc = metrics['bestmodel_train/balanced_accuracy'].max()\n",
    "                valid_balanced_acc = metrics['bestmodel_valid/balanced_accuracy'].max()\n",
    "                test_balanced_acc = metrics['bestmodel_test/balanced_accuracy'].max()\n",
    "\n",
    "                experiment_results.append({\n",
    "                    'Dataset': dataset,\n",
    "                    'Experiment': experiment_type,\n",
    "                    'Train Balanced Accuracy': train_balanced_acc,\n",
    "                    'Validation Balanced Accuracy': valid_balanced_acc,\n",
    "                    'Test Balanced Accuracy': test_balanced_acc\n",
    "                })\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "grouped_results = results_df.groupby(['Dataset', 'Experiment']).mean()\n",
    "grouped_results = grouped_results.reset_index()\n",
    "print(grouped_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo script to analyze experiments results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average across all 25 experiments:\n",
      "train_balanced_accs mean: 56.55\n",
      "valid_balanced_acc mean: 48.04\n",
      "test_balanced_acc mean: 54.37\n",
      "Average across all 25 experiments:\n",
      "train_balanced_accs mean: 58.97\n",
      "valid_balanced_acc mean: 53.39\n",
      "test_balanced_acc mean: 56.19\n"
     ]
    }
   ],
   "source": [
    "experiment_name = \"samplerun1\"\n",
    "# itearte all folders in the experiment folder\n",
    "train_balanced_accs, valid_balanced_acc, test_balanced_acc = [], [], []\n",
    "\n",
    "for folder in os.listdir(os.path.join('logs', experiment_name)):\n",
    "\tmetrics = pd.read_csv(os.path.join('logs', experiment_name, folder, 'metrics.csv'))\n",
    "\n",
    "\t# each column 'bestmodel_train/bestmodel_valid/bestmodel_test' contains only one non-zero entry\n",
    "\t# \twhich is computed at the end of the experiment\n",
    "\ttrain_balanced_accs.append(metrics['bestmodel_train/balanced_accuracy'].max())\n",
    "\tvalid_balanced_acc.append(metrics['bestmodel_valid/balanced_accuracy'].max())\n",
    "\ttest_balanced_acc.append(metrics['bestmodel_test/balanced_accuracy'].max())\n",
    "\n",
    "print(f\"Average across all {len(train_balanced_accs)} experiments:\")\n",
    "print(f\"train_balanced_accs mean: %.2f\" % ((sum(train_balanced_accs)/len(train_balanced_accs))*100))\n",
    "print(f\"valid_balanced_acc mean: %.2f\" % ((sum(valid_balanced_acc)/len(valid_balanced_acc))*100))\n",
    "print(f\"test_balanced_acc mean: %.2f\" % ((sum(test_balanced_acc)/len(test_balanced_acc))*100))\n",
    "\n",
    "experiment_name = \"samplerun2\"\n",
    "# itearte all folders in the experiment folder\n",
    "train_balanced_accs, valid_balanced_acc, test_balanced_acc = [], [], []\n",
    "\n",
    "for folder in os.listdir(os.path.join('logs', experiment_name)):\n",
    "\tmetrics = pd.read_csv(os.path.join('logs', experiment_name, folder, 'metrics.csv'))\n",
    "\n",
    "\t# each column 'bestmodel_train/bestmodel_valid/bestmodel_test' contains only one non-zero entry\n",
    "\t# \twhich is computed at the end of the experiment\n",
    "\ttrain_balanced_accs.append(metrics['bestmodel_train/balanced_accuracy'].max())\n",
    "\tvalid_balanced_acc.append(metrics['bestmodel_valid/balanced_accuracy'].max())\n",
    "\ttest_balanced_acc.append(metrics['bestmodel_test/balanced_accuracy'].max())\n",
    "\n",
    "print(f\"Average across all {len(train_balanced_accs)} experiments:\")\n",
    "print(f\"train_balanced_accs mean: %.2f\" % ((sum(train_balanced_accs)/len(train_balanced_accs))*100))\n",
    "print(f\"valid_balanced_acc mean: %.2f\" % ((sum(valid_balanced_acc)/len(valid_balanced_acc))*100))\n",
    "print(f\"test_balanced_acc mean: %.2f\" % ((sum(test_balanced_acc)/len(test_balanced_acc))*100))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ba94622b8cce9d72f41975dfa05399a969597365498ddf2f52110973e858095a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('low-data')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
